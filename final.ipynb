{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Function to download and save a sample of a given language corpus\n",
    "def save_language_sample(language_code, output_file, num_examples=700):\n",
    "    # Load dataset with streaming enabled for efficient processing\n",
    "    dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{language_code}\", split=\"train\", trust_remote_code=True, streaming=True)\n",
    "    \n",
    "    # Save a limited number of examples to the specified output file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, example in enumerate(dataset):\n",
    "            f.write(example[\"text\"] + \"\\n\")\n",
    "            if i >= num_examples - 1:  # Save only the specified number of examples\n",
    "                break\n",
    "    print(f\"Saved a sample of {language_code.upper()} corpus to {output_file}\")\n",
    "\n",
    "# Save samples for English, French, and Spanish\n",
    "save_language_sample(\"en\", \"english_sample.txt\")\n",
    "save_language_sample(\"fr\", \"french_sample.txt\")\n",
    "save_language_sample(\"es\", \"spanish_sample.txt\")              \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install pytorch_lightning\n",
    "!pip install datasets\n",
    "!pip install nltk\n",
    "!pip install sentencepiece\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "!pip install \"accelerate>=0.26.0\"\n",
    "\n",
    "\n",
    "\n",
    "%pip install evaluate\n",
    "\n",
    "\n",
    "\n",
    "%pip install transformers\n",
    "%pip install torch \n",
    "%pip install torchtext\n",
    "%pip install datasets\n",
    "%pip install numpy tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define a Dataset class for loading data from text files\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=128, fraction=1.0):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        selected_size = int(len(lines) * fraction)\n",
    "        self.lines = random.sample(lines, selected_size)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        line = self.lines[idx].strip()\n",
    "        encoding = self.tokenizer(line, return_tensors=\"pt\", max_length=self.max_length, padding=\"max_length\", truncation=True)\n",
    "        return encoding[\"input_ids\"].squeeze(), encoding[\"attention_mask\"].squeeze()\n",
    "\n",
    "# Load the tokenizers and models for each language pair\n",
    "tokenizers = {\n",
    "    \"en-fr\": MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\"),\n",
    "    \"en-es\": MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\"),\n",
    "    \"fr-es\": MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-fr-es\"),\n",
    "    \"fr-en\": MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\"),  # Reverse tokenizer for fr to en\n",
    "    \"es-en\": MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-es-en\"),  # Reverse tokenizer for es to en\n",
    "    \"es-fr\": MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-es-fr\"),  # Reverse tokenizer for es to fr\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"en-fr\": MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\").to(device),\n",
    "    \"en-es\": MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\").to(device),\n",
    "    \"fr-es\": MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-fr-es\").to(device),\n",
    "    \"fr-en\": MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\").to(device),  # Reverse model for fr to en\n",
    "    \"es-en\": MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-es-en\").to(device),  # Reverse model for es to en\n",
    "    \"es-fr\": MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-es-fr\").to(device),  # Reverse model for es to fr\n",
    "}\n",
    "\n",
    "# Load the datasets with a fraction parameter (e.g., 10% of the original dataset)\n",
    "english_dataset = TextDataset(\"english_sample.txt\", tokenizers[\"en-fr\"], fraction=0.1)\n",
    "french_dataset = TextDataset(\"french_sample.txt\", tokenizers[\"en-fr\"], fraction=0.1)\n",
    "spanish_dataset = TextDataset(\"spanish_sample.txt\", tokenizers[\"en-es\"], fraction=0.1)\n",
    "\n",
    "# Define dataloaders for each language\n",
    "batch_size = 8\n",
    "english_loader = DataLoader(english_dataset, batch_size=batch_size, shuffle=True)\n",
    "french_loader = DataLoader(french_dataset, batch_size=batch_size, shuffle=True)\n",
    "spanish_loader = DataLoader(spanish_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(models[\"en-fr\"].parameters()) + \n",
    "    list(models[\"en-es\"].parameters()) + \n",
    "    list(models[\"fr-es\"].parameters()) + \n",
    "    list(models[\"fr-en\"].parameters()) + \n",
    "    list(models[\"es-en\"].parameters()) + \n",
    "    list(models[\"es-fr\"].parameters()), \n",
    "    lr=1e-5\n",
    ")\n",
    "\n",
    "# Function for denoising (adding noise to a sentence)\n",
    "def add_noise(input_ids, noise_prob=0.1):\n",
    "    noisy_input_ids = input_ids.clone()\n",
    "    for i in range(input_ids.size(0)):\n",
    "        for j in range(input_ids.size(1)):\n",
    "            if random.random() < noise_prob:\n",
    "                noisy_input_ids[i, j] = tokenizers[\"en-fr\"].pad_token_id  # Mask some tokens as padding for noise\n",
    "    return noisy_input_ids\n",
    "\n",
    "# Function for back-translation training step\n",
    "def back_translate_step(source_ids, source_mask, source_lang, target_lang):\n",
    "    # Select the appropriate model and tokenizer\n",
    "    model = models[f\"{source_lang}-{target_lang}\"]\n",
    "    tokenizer = tokenizers[f\"{source_lang}-{target_lang}\"]\n",
    "\n",
    "    # Generate translation\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(source_ids, attention_mask=source_mask)\n",
    "    \n",
    "    # Tokenize back in the target language\n",
    "    target_ids = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    target_ids = tokenizer(target_ids, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "    # Translate back to the source language using reverse model\n",
    "    reverse_model = models[f\"{target_lang}-{source_lang}\"]\n",
    "    generated_back = reverse_model.generate(target_ids)\n",
    "    return generated_back\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    for model in models.values():\n",
    "        model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, (english_batch, french_batch, spanish_batch) in enumerate(zip(english_loader, french_loader, spanish_loader)):\n",
    "        \n",
    "        # Get English data and add noise\n",
    "        eng_ids, eng_mask = english_batch[0].to(device), english_batch[1].to(device)\n",
    "        eng_ids_noisy = add_noise(eng_ids)\n",
    "        \n",
    "        # Forward pass (denoising autoencoding for English)\n",
    "        outputs = models[\"en-fr\"](input_ids=eng_ids_noisy, attention_mask=eng_mask, labels=eng_ids)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Debug statements\n",
    "        print(f\"Epoch {epoch+1}, Step {step+1} - English Denoising Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Back-translation English to French to English\n",
    "        fr_ids = back_translate_step(eng_ids, eng_mask, source_lang=\"en\", target_lang=\"fr\")\n",
    "        outputs = models[\"fr-en\"](input_ids=fr_ids, labels=eng_ids)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Debug statement\n",
    "        print(f\"Epoch {epoch+1}, Step {step+1} - English to French to English Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Repeat for French and Spanish datasets\n",
    "        fr_ids, fr_mask = french_batch[0].to(device), french_batch[1].to(device)\n",
    "        fr_ids_noisy = add_noise(fr_ids)\n",
    "        outputs = models[\"fr-es\"](input_ids=fr_ids_noisy, attention_mask=fr_mask, labels=fr_ids)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Debug statement\n",
    "        print(f\"Epoch {epoch+1}, Step {step+1} - French Denoising Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Back-translation French to Spanish to French\n",
    "        sp_ids = back_translate_step(fr_ids, fr_mask, source_lang=\"fr\", target_lang=\"es\")\n",
    "        outputs = models[\"es-fr\"](input_ids=sp_ids, labels=fr_ids)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Debug statement\n",
    "        print(f\"Epoch {epoch+1}, Step {step+1} - French to Spanish to French Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Same process for Spanish\n",
    "        sp_ids, sp_mask = spanish_batch[0].to(device), spanish_batch[1].to(device)\n",
    "        sp_ids_noisy = add_noise(sp_ids)\n",
    "        outputs = models[\"es-en\"](input_ids=sp_ids_noisy, attention_mask=sp_mask, labels=sp_ids)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Debug statement\n",
    "        print(f\"Epoch {epoch+1}, Step {step+1} - Spanish Denoising Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Back-translation Spanish to English to Spanish\n",
    "        eng_ids = back_translate_step(sp_ids, sp_mask, source_lang=\"es\", target_lang=\"en\")\n",
    "        outputs = models[\"en-es\"](input_ids=eng_ids, labels=sp_ids)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Debug statement\n",
    "        print(f\"Epoch {epoch+1}, Step {step+1} - Spanish to English to Spanish Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(english_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save models \n",
    "model_save_paths = { \"en-fr\": \"marianmt_en_fr.pth\",\n",
    " \"en-es\": \"marianmt_en_es.pth\",\n",
    " \"fr-es\": \"marianmt_fr_es.pth\",\n",
    " \"fr-en\": \"marianmt_fr_en.pth\",\n",
    " \"es-en\": \"marianmt_es_en.pth\",\n",
    " \"es-fr\": \"marianmt_es_fr.pth\", } \n",
    "for lang_pair, model in models.items():\n",
    "    torch.save(model.state_dict(), model_save_paths[lang_pair]) \n",
    "    print(f\"Saved {lang_pair} model to {model_save_paths[lang_pair]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "\n",
    "import sacrebleu\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "from sacrebleu.metrics import TER\n",
    "import random\n",
    "\n",
    "# Load test data with fraction sampling\n",
    "def load_test_data(file_path, fraction=0.1):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    # Select a fraction of the dataset\n",
    "    selected_size = int(len(lines) * fraction)\n",
    "    lines = random.sample(lines, selected_size)\n",
    "    \n",
    "    source_sentences = []\n",
    "    target_sentences = []\n",
    "    for line in lines:\n",
    "        source, target = line.strip().split(\" ||| \")\n",
    "        source_sentences.append(source)\n",
    "        target_sentences.append(target)\n",
    "    return source_sentences, target_sentences\n",
    "\n",
    "# Function to generate translations\n",
    "def generate_translations(source_sentences, tokenizer, model, max_length=128):\n",
    "    translations = []\n",
    "    for sentence in source_sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=max_length, padding=\"max_length\", truncation=True).to(device)\n",
    "        translated_tokens = model.generate(**inputs)\n",
    "        translation = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "        translations.append(translation)\n",
    "    return translations\n",
    "\n",
    "# Function to compute evaluation metrics\n",
    "def evaluate_translations(predictions, references):\n",
    "    # BLEU score\n",
    "    bleu_score = sacrebleu.corpus_bleu(predictions, [references]).score\n",
    "    \n",
    "    # METEOR score (average)\n",
    "    meteor_scores = [\n",
    "        single_meteor_score(ref.split(), pred.split())  # Tokenize each reference and prediction\n",
    "        for ref, pred in zip(references, predictions)\n",
    "    ]\n",
    "    avg_meteor_score = sum(meteor_scores) / len(meteor_scores)\n",
    "    \n",
    "    # TER score\n",
    "    ter_metric = TER()\n",
    "    ter_score = ter_metric.corpus_score(predictions, [references]).score\n",
    "    \n",
    "    return bleu_score, avg_meteor_score, ter_score\n",
    "\n",
    "# Evaluate on each test set\n",
    "test_files = {\n",
    "    \"english_french\": \"english_french_test_data.txt\",\n",
    "    \"english_spanish\": \"english_spanish_test_data.txt\",\n",
    "    \"french_spanish\": \"french_spanish_test_data.txt\"\n",
    "}\n",
    "\n",
    "# Fraction of the dataset to use for testing\n",
    "test_fraction = 0.001  # 5% of each test dataset\n",
    "\n",
    "for test_name, file_path in test_files.items():\n",
    "    print(f\"Evaluating {test_name} translation...\")\n",
    "    \n",
    "    # Load a fraction of the test data\n",
    "    source_sentences, target_sentences = load_test_data(file_path, fraction=test_fraction)\n",
    "    \n",
    "    # Select the appropriate tokenizer and model for the test language pair\n",
    "    if test_name == \"english_french\":\n",
    "        tokenizer = tokenizers[\"en-fr\"]\n",
    "        model = models[\"en-fr\"]\n",
    "    elif test_name == \"english_spanish\":\n",
    "        tokenizer = tokenizers[\"en-es\"]\n",
    "        model = models[\"en-es\"]\n",
    "    elif test_name == \"french_spanish\":\n",
    "        tokenizer = tokenizers[\"fr-es\"]\n",
    "        model = models[\"fr-es\"]\n",
    "    \n",
    "    # Generate translations\n",
    "    predictions = generate_translations(source_sentences, tokenizer, model)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    bleu, meteor, ter = evaluate_translations(predictions, target_sentences)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{test_name} Evaluation Results:\")\n",
    "    print(f\"BLEU Score: {bleu:.2f}\")\n",
    "    print(f\"METEOR Score: {meteor:.2f}\")\n",
    "    print(f\"TER Score: {ter:.2f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
